{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires numpy, tensorflow, keras, OpenCV and matplotlib libraries being installed on your machine. <br>\n",
    "If executing without creating snapshot files beforehand, as described in <a href=\"https://github.com/lizaalert/lacmus/blob/master/docs/train-usage.md\">train-usage.md</a>, compile Cython extentions first by running from the console:\n",
    "```bash\n",
    "python setup.py build_ext --inplace\n",
    "```\n",
    "Or see <a href=\"https://github.com/lizaalert/lacmus/blob/master/docs/train-usage.md#installation\">Installation</a> section in the document above. <br>\n",
    "Make sure the <b>'snapshots'</b> folder contains file with weights of pretrained model. For example, the <b>'resnet50_liza_alert_v1_interface.h5'</b> can be found here: <br>\n",
    "<a href=\"https://github.com/lizaalert/lacmus/releases/tag/0.1.1\">https://github.com/lizaalert/lacmus/releases/tag/0.1.1</a> <br>\n",
    "(Pay attention to the file size!) <br><br>\n",
    "Examples can be found here:<br>\n",
    "<a href=\"https://github.com/lizaalert/lacmus/blob/master/docs/work-demo.md\">https://github.com/lizaalert/lacmus/blob/master/docs/work-demo.md</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "from keras_retinanet.utils.gpu import setup_gpu\n",
    "\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# use this to change which GPU to use\n",
    "gpu = 0\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "setup_gpu(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RetinaNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adjust this to point to your downloaded/trained model\n",
    "# models can be downloaded here: https://github.com/lizaalert/lacmus/releases/tag/0.1.1\n",
    "model_path = os.path.join('snapshots', 'resnet50_liza_alert_v1_interface.h5')\n",
    "\n",
    "# load retinanet model\n",
    "model = models.load_model(model_path, backbone_name='resnet50')\n",
    "\n",
    "# if the model is not converted to an inference model, use the line below\n",
    "#look at: https://github.com/lacmus-foundation/lacmus/blob/master/docs/train-usage.md\n",
    "#model = models.convert_model(model)\n",
    "\n",
    "\n",
    "# load label to names mapping for visualization purposes\n",
    "labels_to_names = {0: 'Pedestrian'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run detection on example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'examples/01.mp4'\n",
    "output_path = 'examples/death_circle_output.avi'\n",
    "fps = 15\n",
    "\n",
    "\n",
    "vcapture = cv2.VideoCapture(video_path)\n",
    "\n",
    "width = int(vcapture.get(cv2.CAP_PROP_FRAME_WIDTH))  # uses given video width and height\n",
    "height = int(vcapture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "vwriter = cv2.VideoWriter(output_path,cv2.VideoWriter_fourcc(*'MJPG'),fps, (width, height)) #\n",
    "\n",
    "num_frames = int(vcapture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"Number of Frames: \", num_frames)\n",
    "print(\"Original Width, Height: \", width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_detection_video(video_path):\n",
    "    count = 0\n",
    "    success = True\n",
    "    start = time.time()\n",
    "    while success:\n",
    "        if count % 100 == 0:\n",
    "            print(\"frame: \", count)\n",
    "        count += 1  # see what frames you are at\n",
    "        # Read next image\n",
    "        success, image = vcapture.read()\n",
    "        \n",
    "        if success:\n",
    "            \n",
    "            # so we can keep orig image scale\n",
    "            draw = image.copy()\n",
    "            draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "            # preprocess image for network\n",
    "            image = preprocess_image(image)\n",
    "            image, scale = resize_image(image)\n",
    "            \n",
    "            # Do compute\n",
    "            boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "            \n",
    "            # correct for image scale\n",
    "            boxes /= scale\n",
    "            \n",
    "            \n",
    "             # visualize detections\n",
    "            for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "                # scores are sorted so we can break\n",
    "                if score < 0.5:\n",
    "                    break\n",
    "\n",
    "                color = label_color(label)\n",
    "\n",
    "                b = box.astype(int)\n",
    "                draw_box(draw, b, color=color)\n",
    "\n",
    "                caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "                draw_caption(draw, b, caption)\n",
    "            \n",
    "            vwriter.write(draw) # overwrites video slice\n",
    "\n",
    "\n",
    "    vcapture.release()\n",
    "    vwriter.release() # \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Total Time: \", end - start)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_detection_video(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
